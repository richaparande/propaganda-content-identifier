{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>730559808</td>\n",
       "      <td>Austrian bishop forcefully rejects German Bish...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>But parents can bless their children.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>789615291</td>\n",
       "      <td>Stop Appeasing the Democrats</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>She told no one about the assault until 30 yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>728972961</td>\n",
       "      <td>FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>I do not destroy the Church.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10562</th>\n",
       "      <td>758386255</td>\n",
       "      <td>Pope Francis vs Contemplative Orders</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Unfortunately, it was about a subject that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11246</th>\n",
       "      <td>780414700</td>\n",
       "      <td>Bishop Morlino Targets ‘Homosexual Subculture’...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>To our seminarians: If you are unchastely prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>698018235</td>\n",
       "      <td>Dan Fishback: It's Okay to Boycott Israeli Pla...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Now he's whining that boycotting his play is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>777488669</td>\n",
       "      <td>The Death Penalty, Instituted by God Himself (...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>If it is ever necessary to hold back the revol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>782149225</td>\n",
       "      <td>Muslim Leader Calls for Conquest of “America, ...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Will Brett Kavanaugh be confirmed to the Supre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>111111132</td>\n",
       "      <td>A popular public school Bible class in West Vi...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>According to Elliott, the Mercer program is “e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>698018235</td>\n",
       "      <td>Dan Fishback: It's Okay to Boycott Israeli Pla...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>\"it’s not that BDS is “censoring” work — it’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>773937361</td>\n",
       "      <td>Things to Do in an Emergency: “What’s Your Tra...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>These are the priests who did make it through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>737194975</td>\n",
       "      <td>Has Trump Opened the Door?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6725</th>\n",
       "      <td>789615291</td>\n",
       "      <td>Stop Appeasing the Democrats</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>From the playground to geopolitics, appeasing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10887</th>\n",
       "      <td>795079843</td>\n",
       "      <td>No sign of breakthrough after Pompeo meets wit...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Many members of Congress have demanded the Whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>700662577</td>\n",
       "      <td>Puerto Rico’s Power Grid Failure Is Causing A ...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>On Friday, former Puerto Rican Governor Alejan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>737255982</td>\n",
       "      <td>J Street \"Kapos\" May Un-Endorse Dem Rep Over \"...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The endorsement, first reported in the Forward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>778139122</td>\n",
       "      <td>Pilger Excoriates Media</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>DB: John, what is the latest we know about how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>775448623</td>\n",
       "      <td>3-D-printed guns put carnage a click away</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Mr. Wilson, an avowed anarchist who hopes for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>777488669</td>\n",
       "      <td>The Death Penalty, Instituted by God Himself (...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The Fifth Commandment later delivered to Moses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>727634031</td>\n",
       "      <td>Ammo seller to Las Vegas killer arrested on fe...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Before the sale, Paddock told him \"that he was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>773937361</td>\n",
       "      <td>Things to Do in an Emergency: “What’s Your Tra...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>If we who still love the Church want priests t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>776616374</td>\n",
       "      <td>54 Years Ago Today: Government &amp; Media Created...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>No, he will not be confirmed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>701837665</td>\n",
       "      <td>The JFK Cover-Up Continues</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>They needed to frame someone for the crime.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8842</th>\n",
       "      <td>762147609</td>\n",
       "      <td>Barack Hussein Obama Hid Efforts to Aid Iran’s...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The Iranian government “uses shell and front c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>999000145</td>\n",
       "      <td>Trump: \"If You Don't Want To Be Saying The Wor...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Trump also encouraged the crowd to vote for Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>730237078</td>\n",
       "      <td>NASA releases images captured at a record-brea...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Right around 3.79 billion miles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4509</th>\n",
       "      <td>7709564349</td>\n",
       "      <td>Trump Pardons Hammonds!</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>You also agree to this site's Privacy Policy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>765982381</td>\n",
       "      <td>Julian Assange</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>“There were a lot of us in the run-up to Sept....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>773937361</td>\n",
       "      <td>Things to Do in an Emergency: “What’s Your Tra...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Do you know anything about what to feed a shee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>999001188</td>\n",
       "      <td>Dear Illegal Alien ‘Caravan’: Say, ‘Hola’ To T...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>We won’t be seeing anything near the same brav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10655</th>\n",
       "      <td>758386255</td>\n",
       "      <td>Pope Francis vs Contemplative Orders</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>I think we can find an almost perfect analogy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>741802985</td>\n",
       "      <td>Willfully Ignorant FBI Can't Discover Motive F...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>It doesn’t admit that there is a war going on,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>766942310</td>\n",
       "      <td>Who ‘Won’ the Trump-Kim Summit?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>War at the Top of the ... Eric Margolis Best P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10816</th>\n",
       "      <td>772947654</td>\n",
       "      <td>Newt Gingrich: The truth about Trump, Putin, a...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Despite the hysteria of the left, it is imposs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8399</th>\n",
       "      <td>7384471099</td>\n",
       "      <td>From Bad To Worse?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Incredible.” Snowden tweeted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>773937361</td>\n",
       "      <td>Things to Do in an Emergency: “What’s Your Tra...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>We could even perhaps buy a few convents so co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7109</th>\n",
       "      <td>703821117</td>\n",
       "      <td>The Cunning CIA</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Reprinted with permission from The Future of F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9317</th>\n",
       "      <td>758756657</td>\n",
       "      <td>Islamizing the Schools: The Case of West Virginia</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>If the school exercise is requiring students t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7804</th>\n",
       "      <td>790677230</td>\n",
       "      <td>Kavanaugh's Nomination Saved?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>When I did at least okay enough at the hearing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>765385479</td>\n",
       "      <td>Julian Assange</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Assange does not ask for special treatment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8810</th>\n",
       "      <td>740356006</td>\n",
       "      <td>Rep. Danny Davis was For/Against/For/Against F...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The term \"Jewish Question\" is largely used by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7141</th>\n",
       "      <td>761334950</td>\n",
       "      <td>A Dismal Record: Why Are They Destroying the N...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The reports from each region showed that many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>728972961</td>\n",
       "      <td>FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>And it all depends on this Truth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>774904810</td>\n",
       "      <td>Army Colonel: False Flag ‘Gulf of Tonkin Incid...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>take our poll - story continues below</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>790266787</td>\n",
       "      <td>Avenatti’s Freak Show</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>As expected, at today’s hearing Kavanaugh will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>111111134</td>\n",
       "      <td>Paul Manafort Secretly Met With Julian Assange...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Assange’s Ecuadorean lawyer, Carlos Poveda, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7144</th>\n",
       "      <td>761334950</td>\n",
       "      <td>A Dismal Record: Why Are They Destroying the N...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>As a snapshot of the current condition of Carm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>706600938</td>\n",
       "      <td>Did Saint Francis Predict Pope Francis?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>De Controversiis on the Roman Pontiff, trans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>728972961</td>\n",
       "      <td>FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The Instructions of the Carbonari say the bish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>7384471099</td>\n",
       "      <td>From Bad To Worse?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Glenn Greenwald, author of the above piece at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>780414700</td>\n",
       "      <td>Bishop Morlino Targets ‘Homosexual Subculture’...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Again, in addition to injuring individuals, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10310</th>\n",
       "      <td>999001619</td>\n",
       "      <td>Guardian ups its vilification of Julian Assange</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>According to Greenwald: “If Paul Manafort … vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>111111113</td>\n",
       "      <td>Kate Steinle's death at the hands of a Mexican...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The jury convicted him of the lesser charge of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>111111132</td>\n",
       "      <td>A popular public school Bible class in West Vi...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>You need to be reading the Bible.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>754179642</td>\n",
       "      <td>New outbreak of Ebola kills 17 in northwest DR...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>More than 99 percent of victims were in the th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>728972961</td>\n",
       "      <td>FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Then, the discourse of the Nuncio was the disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6543</th>\n",
       "      <td>7666320169</td>\n",
       "      <td>Muslim Cleric: “There Were Jews In Islamic Cou...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Whom do you consider to be the most corrupt De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9416</th>\n",
       "      <td>729578579</td>\n",
       "      <td>SPECIAL REPORT: Papal Cover-up Alleged, Pope A...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The best thing to do if someone believes it’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>795379208</td>\n",
       "      <td>Pompeo offers defense for Saudi rulers as Trum...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Saudi leaders for two weeks denied knowing any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8822</th>\n",
       "      <td>740356006</td>\n",
       "      <td>Rep. Danny Davis was For/Against/For/Against F...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Whoever the JTA has on the \"explain away Farra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11464 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id                                      article_title  \\\n",
       "4634    730559808  Austrian bishop forcefully rejects German Bish...   \n",
       "6749    789615291                       Stop Appeasing the Democrats   \n",
       "4998    728972961  FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...   \n",
       "10562   758386255               Pope Francis vs Contemplative Orders   \n",
       "11246   780414700  Bishop Morlino Targets ‘Homosexual Subculture’...   \n",
       "7974    698018235  Dan Fishback: It's Okay to Boycott Israeli Pla...   \n",
       "11343   777488669  The Death Penalty, Instituted by God Himself (...   \n",
       "3203    782149225  Muslim Leader Calls for Conquest of “America, ...   \n",
       "2621    111111132  A popular public school Bible class in West Vi...   \n",
       "7977    698018235  Dan Fishback: It's Okay to Boycott Israeli Pla...   \n",
       "2346    773937361  Things to Do in an Emergency: “What’s Your Tra...   \n",
       "3679    737194975                         Has Trump Opened the Door?   \n",
       "6725    789615291                       Stop Appeasing the Democrats   \n",
       "10887   795079843  No sign of breakthrough after Pompeo meets wit...   \n",
       "389     700662577  Puerto Rico’s Power Grid Failure Is Causing A ...   \n",
       "2686    737255982  J Street \"Kapos\" May Un-Endorse Dem Rep Over \"...   \n",
       "1029    778139122                            Pilger Excoriates Media   \n",
       "9       775448623          3-D-printed guns put carnage a click away   \n",
       "11309   777488669  The Death Penalty, Instituted by God Himself (...   \n",
       "5838    727634031  Ammo seller to Las Vegas killer arrested on fe...   \n",
       "2368    773937361  Things to Do in an Emergency: “What’s Your Tra...   \n",
       "9798    776616374  54 Years Ago Today: Government & Media Created...   \n",
       "6243    701837665                         The JFK Cover-Up Continues   \n",
       "8842    762147609  Barack Hussein Obama Hid Efforts to Aid Iran’s...   \n",
       "3638    999000145  Trump: \"If You Don't Want To Be Saying The Wor...   \n",
       "3476    730237078  NASA releases images captured at a record-brea...   \n",
       "4509   7709564349                            Trump Pardons Hammonds!   \n",
       "8059    765982381                                     Julian Assange   \n",
       "2412    773937361  Things to Do in an Emergency: “What’s Your Tra...   \n",
       "2171    999001188  Dear Illegal Alien ‘Caravan’: Say, ‘Hola’ To T...   \n",
       "...           ...                                                ...   \n",
       "10655   758386255               Pope Francis vs Contemplative Orders   \n",
       "9645    741802985  Willfully Ignorant FBI Can't Discover Motive F...   \n",
       "8130    766942310                    Who ‘Won’ the Trump-Kim Summit?   \n",
       "10816   772947654  Newt Gingrich: The truth about Trump, Putin, a...   \n",
       "8399   7384471099                                 From Bad To Worse?   \n",
       "2372    773937361  Things to Do in an Emergency: “What’s Your Tra...   \n",
       "7109    703821117                                    The Cunning CIA   \n",
       "9317    758756657  Islamizing the Schools: The Case of West Virginia   \n",
       "7804    790677230                      Kavanaugh's Nomination Saved?   \n",
       "2700    765385479                                     Julian Assange   \n",
       "8810    740356006  Rep. Danny Davis was For/Against/For/Against F...   \n",
       "7141    761334950  A Dismal Record: Why Are They Destroying the N...   \n",
       "5293    728972961  FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...   \n",
       "981     774904810  Army Colonel: False Flag ‘Gulf of Tonkin Incid...   \n",
       "1976    790266787                              Avenatti’s Freak Show   \n",
       "1914    111111134  Paul Manafort Secretly Met With Julian Assange...   \n",
       "7144    761334950  A Dismal Record: Why Are They Destroying the N...   \n",
       "3923    706600938            Did Saint Francis Predict Pope Francis?   \n",
       "5022    728972961  FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...   \n",
       "8416   7384471099                                 From Bad To Worse?   \n",
       "11264   780414700  Bishop Morlino Targets ‘Homosexual Subculture’...   \n",
       "10310   999001619    Guardian ups its vilification of Julian Assange   \n",
       "10383   111111113  Kate Steinle's death at the hands of a Mexican...   \n",
       "2556    111111132  A popular public school Bible class in West Vi...   \n",
       "3612    754179642  New outbreak of Ebola kills 17 in northwest DR...   \n",
       "5076    728972961  FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...   \n",
       "6543   7666320169  Muslim Cleric: “There Were Jews In Islamic Cou...   \n",
       "9416    729578579  SPECIAL REPORT: Papal Cover-up Alleged, Pope A...   \n",
       "2230    795379208  Pompeo offers defense for Saudi rulers as Trum...   \n",
       "8822    740356006  Rep. Danny Davis was For/Against/For/Against F...   \n",
       "\n",
       "                label                                      sentence_text  \n",
       "4634   non-propaganda              But parents can bless their children.  \n",
       "6749   non-propaganda  She told no one about the assault until 30 yea...  \n",
       "4998   non-propaganda                       I do not destroy the Church.  \n",
       "10562  non-propaganda  Unfortunately, it was about a subject that the...  \n",
       "11246  non-propaganda  To our seminarians: If you are unchastely prop...  \n",
       "7974       propaganda  Now he's whining that boycotting his play is a...  \n",
       "11343  non-propaganda  If it is ever necessary to hold back the revol...  \n",
       "3203   non-propaganda  Will Brett Kavanaugh be confirmed to the Supre...  \n",
       "2621       propaganda  According to Elliott, the Mercer program is “e...  \n",
       "7977   non-propaganda  \"it’s not that BDS is “censoring” work — it’s ...  \n",
       "2346       propaganda  These are the priests who did make it through ...  \n",
       "3679   non-propaganda                                         California  \n",
       "6725   non-propaganda  From the playground to geopolitics, appeasing ...  \n",
       "10887  non-propaganda  Many members of Congress have demanded the Whi...  \n",
       "389    non-propaganda  On Friday, former Puerto Rican Governor Alejan...  \n",
       "2686   non-propaganda  The endorsement, first reported in the Forward...  \n",
       "1029   non-propaganda  DB: John, what is the latest we know about how...  \n",
       "9          propaganda  Mr. Wilson, an avowed anarchist who hopes for ...  \n",
       "11309  non-propaganda  The Fifth Commandment later delivered to Moses...  \n",
       "5838   non-propaganda  Before the sale, Paddock told him \"that he was...  \n",
       "2368   non-propaganda  If we who still love the Church want priests t...  \n",
       "9798   non-propaganda                      No, he will not be confirmed.  \n",
       "6243   non-propaganda        They needed to frame someone for the crime.  \n",
       "8842   non-propaganda  The Iranian government “uses shell and front c...  \n",
       "3638   non-propaganda  Trump also encouraged the crowd to vote for Mi...  \n",
       "3476   non-propaganda                   Right around 3.79 billion miles.  \n",
       "4509   non-propaganda  You also agree to this site's Privacy Policy a...  \n",
       "8059       propaganda  “There were a lot of us in the run-up to Sept....  \n",
       "2412   non-propaganda  Do you know anything about what to feed a shee...  \n",
       "2171   non-propaganda  We won’t be seeing anything near the same brav...  \n",
       "...               ...                                                ...  \n",
       "10655      propaganda  I think we can find an almost perfect analogy ...  \n",
       "9645   non-propaganda  It doesn’t admit that there is a war going on,...  \n",
       "8130   non-propaganda  War at the Top of the ... Eric Margolis Best P...  \n",
       "10816      propaganda  Despite the hysteria of the left, it is imposs...  \n",
       "8399   non-propaganda                      Incredible.” Snowden tweeted.  \n",
       "2372   non-propaganda  We could even perhaps buy a few convents so co...  \n",
       "7109   non-propaganda  Reprinted with permission from The Future of F...  \n",
       "9317   non-propaganda  If the school exercise is requiring students t...  \n",
       "7804   non-propaganda  When I did at least okay enough at the hearing...  \n",
       "2700   non-propaganda        Assange does not ask for special treatment.  \n",
       "8810   non-propaganda  The term \"Jewish Question\" is largely used by ...  \n",
       "7141   non-propaganda  The reports from each region showed that many ...  \n",
       "5293   non-propaganda                  And it all depends on this Truth.  \n",
       "981    non-propaganda              take our poll - story continues below  \n",
       "1976   non-propaganda  As expected, at today’s hearing Kavanaugh will...  \n",
       "1914   non-propaganda  Assange’s Ecuadorean lawyer, Carlos Poveda, sa...  \n",
       "7144       propaganda  As a snapshot of the current condition of Carm...  \n",
       "3923   non-propaganda      De Controversiis on the Roman Pontiff, trans.  \n",
       "5022   non-propaganda  The Instructions of the Carbonari say the bish...  \n",
       "8416   non-propaganda  Glenn Greenwald, author of the above piece at ...  \n",
       "11264  non-propaganda  Again, in addition to injuring individuals, th...  \n",
       "10310      propaganda  According to Greenwald: “If Paul Manafort … vi...  \n",
       "10383  non-propaganda  The jury convicted him of the lesser charge of...  \n",
       "2556       propaganda                 You need to be reading the Bible.”  \n",
       "3612   non-propaganda  More than 99 percent of victims were in the th...  \n",
       "5076   non-propaganda  Then, the discourse of the Nuncio was the disc...  \n",
       "6543   non-propaganda  Whom do you consider to be the most corrupt De...  \n",
       "9416   non-propaganda  The best thing to do if someone believes it’s ...  \n",
       "2230   non-propaganda  Saudi leaders for two weeks denied knowing any...  \n",
       "8822   non-propaganda  Whoever the JTA has on the \"explain away Farra...  \n",
       "\n",
       "[11464 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df = pd.read_table('coursework2_train.tsv')\n",
    "df = shuffle(df) # randomly shuffle data entries \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size: 11464, label type num: 2\n"
     ]
    }
   ],
   "source": [
    "# take a look at data size\n",
    "raw_labels = df.label.values.tolist()\n",
    "docs = df.sentence_text.tolist()\n",
    "titles = df.article_title.values.tolist()\n",
    "\n",
    "label_dic = {'non-propaganda':0, 'propaganda':1}\n",
    "\n",
    "assert len(docs) == len(raw_labels) == len(titles)\n",
    "labels = [label_dic[rl] for rl in raw_labels] # transfer raw labels (strings) to integer numbers\n",
    "print('total data size: {}, label type num: {}'.format(len(docs), len(label_dic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right around 3.79 billion miles.\n",
      "NASA releases images captured at a record-breaking 3.79 billion miles from Earth\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# take a look at some sentences in the dataset\n",
    "print(docs[25])\n",
    "print(titles[25])\n",
    "print(labels[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use both titles and sentences to train my model, as both together would provide more information to detect whether the post is propaganda or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first piece of preprocessing I performed is converting the text to lowercase, in order to remove stopwords later (nltk's stopwords are all in lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to lowercase\n",
    "df['article_title'] = df['article_title'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['sentence_text'] = df['sentence_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, I lemmatized the text so word importances can be correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df['article_title'] = df['article_title'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(y) for y in x.split()]))\n",
    "df['sentence_text'] = df['sentence_text'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then removed all punctuation, in order to make clean and word-only tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "import re\n",
    "df['article_title'] = df['article_title'].apply(lambda x: re.sub('[^\\w\\s]', '', x))\n",
    "df['sentence_text'] = df['sentence_text'].apply(lambda x: re.sub('[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I tokenized the text to perform analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "from nltk import word_tokenize\n",
    "df['tokens_title'] = df['article_title'].apply(lambda x: nltk.word_tokenize(x))\n",
    "df['tokens_sentence'] = df['sentence_text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>tokens_title</th>\n",
       "      <th>tokens_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>730559808</td>\n",
       "      <td>austrian bishop forcefully reject german bisho...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>but parent can bless their children</td>\n",
       "      <td>[austrian, bishop, forcefully, reject, german,...</td>\n",
       "      <td>[but, parent, can, bless, their, children]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>789615291</td>\n",
       "      <td>stop appeasing the democrat</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>she told no one about the assault until 30 yea...</td>\n",
       "      <td>[stop, appeasing, the, democrat]</td>\n",
       "      <td>[she, told, no, one, about, the, assault, unti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>728972961</td>\n",
       "      <td>for the first time online archbishop lefebvres...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>i do not destroy the church</td>\n",
       "      <td>[for, the, first, time, online, archbishop, le...</td>\n",
       "      <td>[i, do, not, destroy, the, church]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10562</th>\n",
       "      <td>758386255</td>\n",
       "      <td>pope francis v contemplative order</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>unfortunately it wa about a subject that the w...</td>\n",
       "      <td>[pope, francis, v, contemplative, order]</td>\n",
       "      <td>[unfortunately, it, wa, about, a, subject, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11246</th>\n",
       "      <td>780414700</td>\n",
       "      <td>bishop morlino target homosexual subculture in...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>to our seminarians if you are unchastely propo...</td>\n",
       "      <td>[bishop, morlino, target, homosexual, subcultu...</td>\n",
       "      <td>[to, our, seminarians, if, you, are, unchastel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id                                      article_title  \\\n",
       "4634    730559808  austrian bishop forcefully reject german bisho...   \n",
       "6749    789615291                        stop appeasing the democrat   \n",
       "4998    728972961  for the first time online archbishop lefebvres...   \n",
       "10562   758386255                 pope francis v contemplative order   \n",
       "11246   780414700  bishop morlino target homosexual subculture in...   \n",
       "\n",
       "                label                                      sentence_text  \\\n",
       "4634   non-propaganda                but parent can bless their children   \n",
       "6749   non-propaganda  she told no one about the assault until 30 yea...   \n",
       "4998   non-propaganda                        i do not destroy the church   \n",
       "10562  non-propaganda  unfortunately it wa about a subject that the w...   \n",
       "11246  non-propaganda  to our seminarians if you are unchastely propo...   \n",
       "\n",
       "                                            tokens_title  \\\n",
       "4634   [austrian, bishop, forcefully, reject, german,...   \n",
       "6749                    [stop, appeasing, the, democrat]   \n",
       "4998   [for, the, first, time, online, archbishop, le...   \n",
       "10562           [pope, francis, v, contemplative, order]   \n",
       "11246  [bishop, morlino, target, homosexual, subcultu...   \n",
       "\n",
       "                                         tokens_sentence  \n",
       "4634          [but, parent, can, bless, their, children]  \n",
       "6749   [she, told, no, one, about, the, assault, unti...  \n",
       "4998                  [i, do, not, destroy, the, church]  \n",
       "10562  [unfortunately, it, wa, about, a, subject, tha...  \n",
       "11246  [to, our, seminarians, if, you, are, unchastel...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take another quick look at the data to make sure preprocessing is working\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remove stopwords, as including common words would make the models less effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [x for x in text if x not in sw]\n",
    "\n",
    "cleaned_title = df['tokens_title'].apply(lambda x: remove_stopwords(x))\n",
    "cleaned_sentence = df['tokens_sentence'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the sentences in the correct format for developing the models\n",
    "final_title = [' '.join(x) for x in cleaned_title]\n",
    "final_sentence = [' '.join(x) for x in cleaned_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austrian bishop forcefully reject german bishops idea blessing homosexual union parent bless children'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine titles and sentences to form one sentence\n",
    "concat_lists = lambda x,y: x + \" \" + y\n",
    "final_text = list(map(concat_lists, final_title, final_sentence))\n",
    "final_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split the data. I chose a 60/20/20 split for the train, dev, and test sets as this is known to be a fair split in machine learning processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 6878, dev size: 2293, test size: 2292\n"
     ]
    }
   ],
   "source": [
    "# split the data into train, dev and test\n",
    "train_ratio, dev_ratio, test_ratio = 0.6, 0.2, 0.2\n",
    "train_docs = final_text[:int(len(final_text)*train_ratio)]\n",
    "train_labels = labels[:int(len(final_text)*train_ratio)]\n",
    "\n",
    "dev_docs = final_text[int(len(final_text)*train_ratio):int(len(final_text)*(train_ratio+dev_ratio))]\n",
    "dev_labels = labels[int(len(final_text)*train_ratio):int(len(final_text)*(train_ratio+dev_ratio))]\n",
    "\n",
    "test_docs = final_text[-int(len(final_text)*(test_ratio)):]\n",
    "test_labels = labels[-int(len(final_text)*(test_ratio)):]\n",
    "\n",
    "print('train size: {}, dev size: {}, test size: {}'.format(len(train_labels), len(dev_labels), len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic machine learning model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf is an ideal method to use for feature extraction in this text analysis as it would outline the key words required to correctly categorize the data. I had initially decided not to set a max feature number, as limiting the features produced results with a lower accuracy. However, I realized that this led to overfitting of the data, so I used a max feature number of 3000.\n",
    "\n",
    "I believe logistic regression is a good algorithm to use in this case because the labels are binary - propaganda and non-propaganda. Since logistic regression is one of the most common methods for binary classification, this is a safe bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7478184991273996\n",
      "precision: 0.7126680453796216\n",
      "rec: 0.5847357220371788\n",
      "f1: 0.5826031065881093\n"
     ]
    }
   ],
   "source": [
    "# tf-idf and logistic regression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_feature_num = 3000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = train_vectorizer.fit_transform(train_docs)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_docs)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(solver='lbfgs').fit(train_vecs, train_labels)\n",
    "\n",
    "# make prediction\n",
    "test_pred_lr = clf_lr.predict(test_vecs)\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "acc_lr = accuracy_score(test_labels, test_pred_lr)\n",
    "pre_lr, rec_lr, f1_lr, _ = precision_recall_fscore_support(test_labels, test_pred_lr, average='macro')\n",
    "print('accuracy:', acc_lr)\n",
    "print('precision:', pre_lr)\n",
    "print('rec:', rec_lr)\n",
    "print('f1:', f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression provides a macro f1 of 0.583, which is not very high. I then tried a neural-based model (MLP) to try to increase this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network model: Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use GloVe word embeddings, as this model looks at longer range co-occurance rather than simply analyzing words next to each other.\n",
    "I chose the vectors trained on Wikipedia 2014 + Gigaword 5 with 6 billion tokens, as a dataset of 11464 sentences is fairly small and would not require the larger word vector text files.\n",
    "https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the glove pre-trained embedding\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "path_of_downloaded_files = \"/users/richa/glove.6B/glove.6B.300d.txt\"\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_vec_dim = 300\n",
    "oov_vec = np.random.rand(word_vec_dim)\n",
    "\n",
    "def vectorize_sent(word_vectors, sent):\n",
    "    word_vecs = []\n",
    "    for token in word_tokenize(sent): \n",
    "        if token not in word_vectors: \n",
    "            word_vecs.append(oov_vec)\n",
    "        else:\n",
    "            word_vecs.append(word_vectors[token].astype('float64'))\n",
    "    return np.mean(word_vecs,axis=0)\n",
    "\n",
    "# test function to make sure it works\n",
    "vv = vectorize_sent(word_vectors, train_docs[1])\n",
    "print(vv.shape)\n",
    "print(vv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6878, 300)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# create vector representations\n",
    "train_vectors = np.array([vectorize_sent(word_vectors, ss) for ss in train_docs])\n",
    "dev_vectors = np.array([vectorize_sent(word_vectors, ss) for ss in dev_docs])\n",
    "print(train_vectors.shape)\n",
    "print(train_vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple MLP (multi-layer perceptron) as the classification model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, dp_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, input_dim*2)\n",
    "        self.output_layer = nn.Linear(input_dim*2, out_dim)\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "       \n",
    "    def forward(self, x_in):\n",
    "        z1 = self.dropout(x_in) # output of the input layer, after dropout\n",
    "        z2 = self.relu(self.hidden_layer(z1)) # output of the hidden layer\n",
    "        logits = self.output_layer(z2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to test out and tune hyperparameters to find a model with the best accuracy. I chose a fairly standard initial learning rate of 0.005 so that errors can be explored. Since the learning rate decreases with each epoch, this would benefit the model. I initially tried a higher learning rate of 0.01, but this was too high for the model to be efficient.\n",
    "I decided to limit the number of epochs to 30, as I noticed that after a certain point the model's accuracy would drop. A dropout rate of 0.5 seemed to be the best option, as well as a batch size of 128 - this allows the model to train on more samples before updating model parameters, so a higher batch size led to a higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "dropout_rate = 0.5 \n",
    "model = MLP(word_vec_dim,len(label_dic),dropout_rate) \n",
    "loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 30 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 128 # mini batch size\n",
    "lr = 0.005 # initial learning rate\n",
    "\n",
    "# initialize optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9) # decays the learning rate of each parameter group by gamma every step_size epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the macro-f1 on dev set is 0.5256958200114521\n",
      "learning rate 0.005\n",
      "best model updated; new best f1 0.5256958200114521\n",
      "\n",
      "---> after epoch 1 the macro-f1 on dev set is 0.5591363300867644\n",
      "learning rate 0.005\n",
      "best model updated; new best f1 0.5591363300867644\n",
      "\n",
      "---> after epoch 2 the macro-f1 on dev set is 0.5964138609325236\n",
      "learning rate 0.005\n",
      "best model updated; new best f1 0.5964138609325236\n",
      "\n",
      "---> after epoch 3 the macro-f1 on dev set is 0.6137111732430592\n",
      "learning rate 0.005\n",
      "best model updated; new best f1 0.6137111732430592\n",
      "\n",
      "---> after epoch 4 the macro-f1 on dev set is 0.615233487383501\n",
      "learning rate 0.005\n",
      "best model updated; new best f1 0.615233487383501\n",
      "\n",
      "---> after epoch 5 the macro-f1 on dev set is 0.6528440694131449\n",
      "learning rate 0.005\n",
      "best model updated; new best f1 0.6528440694131449\n",
      "\n",
      "---> after epoch 6 the macro-f1 on dev set is 0.6334546100420498\n",
      "learning rate 0.005\n",
      "\n",
      "---> after epoch 7 the macro-f1 on dev set is 0.6399110921412379\n",
      "learning rate 0.005\n",
      "\n",
      "---> after epoch 8 the macro-f1 on dev set is 0.6275511021127164\n",
      "learning rate 0.005\n",
      "\n",
      "---> after epoch 9 the macro-f1 on dev set is 0.6493112565906662\n",
      "learning rate 0.005\n",
      "\n",
      "---> after epoch 10 the macro-f1 on dev set is 0.6492152617763485\n",
      "learning rate 0.0045000000000000005\n",
      "\n",
      "---> after epoch 11 the macro-f1 on dev set is 0.646410848272319\n",
      "learning rate 0.0045000000000000005\n",
      "\n",
      "---> after epoch 12 the macro-f1 on dev set is 0.6575205023333649\n",
      "learning rate 0.0045000000000000005\n",
      "best model updated; new best f1 0.6575205023333649\n",
      "\n",
      "---> after epoch 13 the macro-f1 on dev set is 0.659974991383249\n",
      "learning rate 0.0045000000000000005\n",
      "best model updated; new best f1 0.659974991383249\n",
      "\n",
      "---> after epoch 14 the macro-f1 on dev set is 0.6635864223339268\n",
      "learning rate 0.0045000000000000005\n",
      "best model updated; new best f1 0.6635864223339268\n",
      "\n",
      "---> after epoch 15 the macro-f1 on dev set is 0.663430246047999\n",
      "learning rate 0.0045000000000000005\n",
      "\n",
      "---> after epoch 16 the macro-f1 on dev set is 0.6560032479796278\n",
      "learning rate 0.0045000000000000005\n",
      "\n",
      "---> after epoch 17 the macro-f1 on dev set is 0.6641441265501415\n",
      "learning rate 0.0045000000000000005\n",
      "best model updated; new best f1 0.6641441265501415\n",
      "\n",
      "---> after epoch 18 the macro-f1 on dev set is 0.6747327113042876\n",
      "learning rate 0.0045000000000000005\n",
      "best model updated; new best f1 0.6747327113042876\n",
      "\n",
      "---> after epoch 19 the macro-f1 on dev set is 0.6629518273535111\n",
      "learning rate 0.0045000000000000005\n",
      "\n",
      "---> after epoch 20 the macro-f1 on dev set is 0.6736949587256517\n",
      "learning rate 0.004050000000000001\n",
      "\n",
      "---> after epoch 21 the macro-f1 on dev set is 0.6810775981970014\n",
      "learning rate 0.004050000000000001\n",
      "best model updated; new best f1 0.6810775981970014\n",
      "\n",
      "---> after epoch 22 the macro-f1 on dev set is 0.6678899383099524\n",
      "learning rate 0.004050000000000001\n",
      "\n",
      "---> after epoch 23 the macro-f1 on dev set is 0.6686562638762841\n",
      "learning rate 0.004050000000000001\n",
      "\n",
      "---> after epoch 24 the macro-f1 on dev set is 0.6752634957228403\n",
      "learning rate 0.004050000000000001\n",
      "\n",
      "---> after epoch 25 the macro-f1 on dev set is 0.6789351570591264\n",
      "learning rate 0.004050000000000001\n",
      "\n",
      "---> after epoch 26 the macro-f1 on dev set is 0.6836970411371946\n",
      "learning rate 0.004050000000000001\n",
      "best model updated; new best f1 0.6836970411371946\n",
      "\n",
      "---> after epoch 27 the macro-f1 on dev set is 0.6684457499340687\n",
      "learning rate 0.004050000000000001\n",
      "\n",
      "---> after epoch 28 the macro-f1 on dev set is 0.6695791411606645\n",
      "learning rate 0.004050000000000001\n",
      "\n",
      "---> after epoch 29 the macro-f1 on dev set is 0.6730565105115571\n",
      "learning rate 0.004050000000000001\n"
     ]
    }
   ],
   "source": [
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    for idx in range(0,len(train_vectors),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        x_data = torch.tensor(train_vectors[idx:idx+batch_size], dtype=torch.float)\n",
    "        if x_data.shape[0] == 0: continue\n",
    "        y_target = torch.tensor(train_labels[idx:idx+batch_size], dtype=torch.int64)\n",
    "\n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = model(x_data)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        dev_data = torch.tensor(dev_vectors, dtype=torch.float)\n",
    "        dev_target = torch.tensor(dev_labels, dtype=torch.int64)\n",
    "        dev_prediction = model(dev_data)\n",
    "        pred_labels = [np.argmax(dp.numpy()) for dp in dev_prediction]\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(dev_target, pred_labels, average='macro')\n",
    "        print('\\n---> after epoch {} the macro-f1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best f1',f1)\n",
    "            \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP model provides a best macro f1 of 0.684 on the train/dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then tested my model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-f1 on test data: 0.6836644730734227\n"
     ]
    }
   ],
   "source": [
    "# load the best model weights\n",
    "model.load_state_dict(best_model) \n",
    "test_vectors = np.array([vectorize_sent(word_vectors, ss) for ss in test_docs])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "    test_data = torch.tensor(test_vectors, dtype=torch.float)\n",
    "    test_target = torch.tensor(test_labels, dtype=torch.int64)\n",
    "    test_prediction = model(test_data)\n",
    "    pred_labels = [np.argmax(dp.numpy()) for dp in test_prediction]\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(test_target, pred_labels, average='macro')\n",
    "    print('macro-f1 on test data:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is applied to the test set, the macro-f1 is 0.684. This is higher than the score obtained from the logistic regression model, so this is clearly the better performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_info_to_save = {\n",
    "    'input_dim': word_vec_dim,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'neural_weights': best_model,\n",
    "    'oov_vector': oov_vec,\n",
    "    'n_epochs': n_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr\n",
    "}\n",
    "save_path = open(\"trained_model.pickle\",\"wb\")\n",
    "pickle.dump(all_info_to_save, save_path)\n",
    "save_path.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
